{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "Find items that do not follow the distribution of the majority of data\n",
    "![](images/anomalies.png)\n",
    "*Taken from https://arxiv.org/pdf/1901.03407.pdf*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of models\n",
    "- Semi-supervised\n",
    "- Unsupervised\n",
    "- Hybrid\n",
    "- One-Class Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoder\n",
    "\n",
    "\n",
    "![](images/autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "Time Series Anomaly Detection\n",
    "\n",
    "![](images/time_series.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "Feature Extraction / Data Compression\n",
    "\n",
    "![](images/feature.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "Image Denoising\n",
    "\n",
    "![](images/denoising.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types\n",
    "- Vanilla autoencoder\n",
    "- Deep autoencoder\n",
    "- Convolutional autoencoder\n",
    "- Spati-temporal autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vanilla Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "class Autoencoder(gluon.Block):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "            \n",
    "            # Encoder\n",
    "            self.encoder = gluon.nn.Dense(128, activation='relu')\n",
    "            \n",
    "            # Decoder\n",
    "            self.decoder = gluon.nn.Dense(28 * 28, activation='tanh')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoencoder(gluon.Block):\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "        \n",
    "            # Encoder\n",
    "            self.encoder = gluon.nn.Sequential()\n",
    "            with self.encoder.name_scope():\n",
    "                self.encoder.add(gluon.nn.Dense(128, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.Dense(64, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.Dense(12, activation='relu'))\n",
    "                \n",
    "                #Bottleneck\n",
    "                self.encoder.add(gluon.nn.Dense(3))\n",
    "\n",
    "            # Decoder\n",
    "            self.decoder = gluon.nn.Sequential()\n",
    "            with self.decoder.name_scope():\n",
    "                self.decoder.add(gluon.nn.Dense(12, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.Dense(64, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.Dense(128, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.Dense(28 * 28, activation='tanh'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Convolutional Autoenocoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(gluon.nn.HybridBlock):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        with self.name_scope():\n",
    "            \n",
    "            # Encoder\n",
    "            self.encoder = gluon.nn.HybridSequential(prefix=\"\")\n",
    "            with self.encoder.name_scope():\n",
    "                self.encoder.add(gluon.nn.Conv2D(32, 5, padding=0, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.MaxPool2D(2))\n",
    "                self.encoder.add(gluon.nn.Conv2D(32, 5, padding=0, activation='relu'))\n",
    "                self.encoder.add(gluon.nn.MaxPool2D(2))\n",
    "                self.encoder.add(gluon.nn.Dense(100))\n",
    "            self.decoder = gluon.nn.HybridSequential(prefix=\"\")\n",
    "            \n",
    "            # Decoder\n",
    "            with self.decoder.name_scope():\n",
    "                self.decoder.add(gluon.nn.Dense(32*22*22, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.HybridLambda(lambda F, x: F.UpSampling(x, scale=2, sample_type='nearest')))\n",
    "                self.decoder.add(gluon.nn.Conv2DTranspose(32, 5, activation='relu'))\n",
    "                self.decoder.add(gluon.nn.HybridLambda(lambda F, x: F.UpSampling(x, scale=2, sample_type='nearest')))\n",
    "                self.decoder.add(gluon.nn.Conv2DTranspose(1, kernel_size=5, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder[0](x)\n",
    "        \n",
    "        # need to reshape ouput feature vector from Dense(32*22*22), before it is upsampled\n",
    "        x = x.reshape((-1,32,22,22))\n",
    "        x = self.decoder[1:](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function - Mean Squared Error (L2):\n",
    "- assumes pixel-wise indenpendence\n",
    "\n",
    "<img src=\"images/l2loss.png\" alt=\"drawing\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function - Structural Similarity Index:\n",
    "- structure\n",
    "- contrast\n",
    "- luminance\n",
    "\n",
    "<img src=\"images/ssim.png\" alt=\"drawing\" width=\"1500\"/>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Customize Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSIM(gluon.loss.Loss):\n",
    "    def __init__(self, size = 11, weight=None, batch_axis=0, **kwargs):\n",
    "        super(SSIM, self).__init__(weight, batch_axis, **kwargs)\n",
    "        self.size = size\n",
    "        self.channel = 3\n",
    "        self.window = create_window(size, self.channel)\n",
    "        \n",
    "    def hybrid_forward(self, F, img1, img2):\n",
    "        \n",
    "        # Gaussian Filter\n",
    "        self.window = self.window.as_in_context(img1.context)\n",
    "        \n",
    "        # mean\n",
    "        mu1 = F.Convolution(data=img1, weight=self.window, kernel=(self.size,self.size), no_bias=True, pad=(self.size//2,self.size//2), num_filter=self.channel, num_group = self.channel)\n",
    "        mu2 = F.Convolution(data=img2, weight=self.window, kernel=(self.size,self.size), no_bias=True, pad=(self.size//2,self.size//2), num_filter=self.channel, num_group = self.channel)\n",
    "        \n",
    "        # standard deviation\n",
    "        mu1_sq = F.power(mu1,2)\n",
    "        mu2_sq = F.power(mu2,2)\n",
    "        \n",
    "        mu1_mu2 = mu1*mu2\n",
    "        \n",
    "        sigma1_sq = F.Convolution(img1*img1, weight=self.window, kernel=(self.size,self.size), no_bias=True, pad=(self.size//2,self.size//2), num_filter=self.channel, num_group = self.channel) - mu1_sq\n",
    "        sigma2_sq = F.Convolution(img2*img2, weight=self.window, kernel=(self.size,self.size), no_bias=True, pad=(self.size//2,self.size//2), num_filter=self.channel, num_group = self.channel) - mu2_sq\n",
    "        sigma12 = F.Convolution(img1*img2, weight=self.window, kernel=(self.size,self.size), no_bias=True, pad=(self.size//2,self.size//2), num_filter=self.channel, num_group = self.channel) - mu1_mu2\n",
    "       \n",
    "        # luminance + contrast + structure\n",
    "        ssim_map = ((2*mu1_mu2 + 0.0001)*(2*sigma12 + 2.7e-08))/((mu1_sq + mu2_sq + 0.0001)*(sigma1_sq + sigma2_sq + 2.7e-08))\n",
    "\n",
    "        return ssim_map.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Anomaly Detection with Convolutional Autoencoders\n",
    "\n",
    "UCSD dataset:\n",
    "![](images/ucsd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "![](images/autoencoder_dense.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "Without bottlneck layer, the model can not learn meaningful features:\n",
    "![](images/autoencoder_nodense.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Takeaways\n",
    "\n",
    "No size fits all:\n",
    "\n",
    "- how much variation in the training set\n",
    "- size of anomalies\n",
    "- type of data (image, audio, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anomaly Detection in Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem of the previous model:\n",
    "- we do not assume any dependency/order between images\n",
    "- need to train model on sequence of images\n",
    "- need to use LSTM cells to learn sequence\n",
    "\n",
    "![](images/Picture9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spatio-temporal Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMAE(gluon.nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ConvLSTMAE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "\n",
    "          # Encoder\n",
    "          self.encoder = gluon.nn.HybridSequential()\n",
    "          self.encoder.add(gluon.nn.Conv2D(128, kernel_size=11, strides=4, activation='relu'))\n",
    "          self.encoder.add(gluon.nn.Conv2D(64, kernel_size=5, strides=2, activation='relu'))\n",
    "\n",
    "          # Temporal Encoder\n",
    "          self.temporal_encoder = gluon.rnn.HybridSequentialRNNCell()\n",
    "          self.temporal_encoder.add(gluon.contrib.rnn.Conv2DLSTMCell((64,26,26), 64, 3, 3, i2h_pad=1))\n",
    "          self.temporal_encoder.add(gluon.contrib.rnn.Conv2DLSTMCell((64,26,26), 32, 3, 3, i2h_pad=1))\n",
    "          self.temporal_encoder.add(gluon.contrib.rnn.Conv2DLSTMCell((32,26,26), 64, 3, 3, i2h_pad=1))\n",
    "\n",
    "          # Decoder\n",
    "          self.decoder =  gluon.nn.HybridSequential()\n",
    "          self.decoder.add(gluon.nn.Conv2DTranspose(channels=128, kernel_size=5, strides=2, activation='relu'))\n",
    "          self.decoder.add(gluon.nn.Conv2DTranspose(channels=10, kernel_size=11, strides=4, activation='sigmoid'))\n",
    "\n",
    "    def hybrid_forward(self, F, x, states=None, **kwargs):\n",
    "        x = self.encoder(x)\n",
    "        x, states = self.temporal_encoder(x, states)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional LSTM\n",
    "*Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting* (NIPS 2015)\n",
    "\n",
    "#### Fully Connected LSTM:\n",
    "- powerful for handling temporal data\n",
    "- too much redundancy for spatial data\n",
    "- input-to-state and state-to-state transitions do not encode spatial information\n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
    "*Taken from https://colah.github.io/posts/2015-08-Understanding-LSTMs/* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Convolutional LSTM:\n",
    "- determines future state of a cell based on inputs and past states of its neighbors\n",
    "![](https://cdn-images-1.medium.com/max/640/1*u8neecA4w6b_F1NgnyPP0Q.png)\n",
    "*Taken from https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lab: find anamlous movements and objects in UCSD dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import modules and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "import tarfile\n",
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ctx = mx.cpu()\n",
    "num_epochs = 80\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Download dataset \n",
    "if not os.path.isfile(\"UCSD_Anomaly_Dataset.tar.gz\"):\n",
    "  response = request.urlretrieve(\"http://www.svcl.ucsd.edu/projects/anomaly/UCSD_Anomaly_Dataset.tar.gz\", \"UCSD_Anomaly_Dataset.tar.gz\")\n",
    "  tar = tarfile.open(\"UCSD_Anomaly_Dataset.tar.gz\")\n",
    "  tar.extractall()\n",
    "  tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of image files\n",
    "files = sorted(glob.glob('UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train/*/*'))\n",
    "a = np.zeros((len(files),1,100,100))\n",
    "\n",
    "# Read images\n",
    "for idx, filename in enumerate(files):\n",
    "    im = Image.open(filename)\n",
    "    im = im.resize((100,100))\n",
    "    a[idx,0,:,:] = np.array(im, dtype=np.float32)/255.0\n",
    "\n",
    "# Dataloader\n",
    "dataset = gluon.data.ArrayDataset(mx.nd.array(a, dtype=np.float32))\n",
    "dataloader = gluon.data.DataLoader(dataset, batch_size=batch_size, last_batch='rollover',shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "model = ConvolutionalAutoencoder()\n",
    "model.hybridize()\n",
    "#model.collect_params().initialize(mx.init.Xavier(), ctx=mx.gpu())\n",
    "model.load_parameters(\"autoencoder_ucsd.params\", ctx=ctx)\n",
    "\n",
    "# Loss\n",
    "l2loss = gluon.loss.L2Loss()\n",
    "\n",
    "# Trainer\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 1e-4, 'wd': 1e-5, 'epsilon':1e-6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/80], loss:0.0013\n"
     ]
    }
   ],
   "source": [
    " # Start the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # iterate over bacthes of training images\n",
    "    for image in dataloader:\n",
    "        \n",
    "        # load image on the right context\n",
    "        image  = image.as_in_context(ctx)\n",
    "\n",
    "        with mx.autograd.record():\n",
    "            \n",
    "            # forward pass\n",
    "            reconstructed = model(image)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = l2loss(reconstructed, image)\n",
    "\n",
    "        # backpropagation    \n",
    "        loss.backward()\n",
    "        optimizer.step(batch_size)\n",
    "\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, mx.nd.mean(loss).asscalar()))\n",
    "\n",
    "# save model parameters\n",
    "model.save_parameters(\"mymodel.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Inference\n",
    "![](images/lstmae.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to test images\n",
    "files = sorted(glob.glob('UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test/Test024/*'))\n",
    "\n",
    "a = np.zeros((len(files),1,100,100))\n",
    "\n",
    "# load and resize images\n",
    "for idx,filename in enumerate(files):\n",
    "    im = Image.open(filename)\n",
    "    im = im.resize((100,100))\n",
    "    a[idx,0,:,:] = np.array(im, dtype=np.float32)/255.0\n",
    "\n",
    "# define dataloader\n",
    "dataset = gluon.data.ArrayDataset(mx.nd.array(a, dtype=np.float32))\n",
    "dataloader = gluon.data.DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Helper function for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(img, output, diff, H, threshold):\n",
    "    \n",
    "    fig, (ax0, ax1, ax2,ax3) = plt.subplots(ncols=4, figsize=(10, 5))\n",
    "    ax0.set_axis_off()\n",
    "    ax1.set_axis_off()\n",
    "    ax2.set_axis_off()\n",
    "    ax3.set_axis_off()\n",
    "    \n",
    "    ax0.set_title('input image')\n",
    "    ax1.set_title('reconstructed image')\n",
    "    ax2.set_title('diff ')\n",
    "    ax3.set_title('anomalies')\n",
    "    \n",
    "    ax0.imshow(img, cmap=plt.cm.gray, interpolation='nearest') \n",
    "    ax1.imshow(output, cmap=plt.cm.gray, interpolation='nearest')   \n",
    "    ax2.imshow(diff, cmap=plt.cm.viridis, interpolation='nearest')  \n",
    "    ax3.imshow(img, cmap=plt.cm.gray, interpolation='nearest')\n",
    "    \n",
    "    # find pixel values with high reconstruction error and mark them\n",
    "    x,y = np.where(H > threshold)\n",
    "    ax3.scatter(y,x,color='red',s=0.8) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# iterate over test images\n",
    "for image in dataloader:\n",
    "\n",
    "    # load image on the right context\n",
    "    img = image.as_in_context(mx.cpu())\n",
    "\n",
    "    # inference\n",
    "    output = model(img)\n",
    "\n",
    "    # convert from MXNet NDarray to Numpy array\n",
    "    output = output.asnumpy()\n",
    "    img = image.asnumpy()\n",
    "    \n",
    "    # compute residual map\n",
    "    diff = np.abs(output-img)\n",
    "    \n",
    "    # perform 2d convolution on residual map\n",
    "    tmp = diff[0,0,:,:]\n",
    "    H = signal.convolve2d(tmp, np.ones((8,8)), mode='same')\n",
    " \n",
    "    # plot images\n",
    "    plot(img[0,0,:,:], output[0,0,:,:], diff[0,0,:,:], H, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
