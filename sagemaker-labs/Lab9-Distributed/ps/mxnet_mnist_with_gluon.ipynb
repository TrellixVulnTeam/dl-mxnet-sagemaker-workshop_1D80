{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Training with MXNet and Gluon\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using MXNet and the Gluon API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from mxnet import gluon\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./data/train/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\n",
      "Downloading ./data/train/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading ./data/test/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading ./data/test/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mxnet.gluon.data.vision.datasets.MNIST at 0x7f8b8415c128>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gluon.data.vision.MNIST('./data/train', train=True)\n",
    "gluon.data.vision.MNIST('./data/test', train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training function\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, except that you need to provide a `train` function. The `train` function will check for the validation accuracy at the end of every epoch and checkpoints the best model so far, along with the optimizer state, in the folder `/opt/ml/checkpoints` if the folder path exists, else it will skip the checkpointing. When SageMaker calls your function, it will pass in arguments that describe the training environment. Check the script below to see how this works.\n",
    "\n",
    "The script here is an adaptation of the [Gluon MNIST example](https://github.com/apache/incubator-mxnet/blob/master/example/gluon/mnist.py) provided by the [Apache MXNet](https://mxnet.incubator.apache.org/) project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import argparse\r\n",
      "import logging\r\n",
      "import os\r\n",
      "\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def train(args):\r\n",
      "    # SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\r\n",
      "    # the current container environment, but here we just use simple cpu context.\r\n",
      "    ctx = mx.cpu()\r\n",
      "\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = args.batch_size\r\n",
      "    epochs = args.epochs\r\n",
      "    learning_rate = args.learning_rate\r\n",
      "    momentum = args.momentum\r\n",
      "    log_interval = args.log_interval\r\n",
      "\r\n",
      "    num_gpus = int(os.environ['SM_NUM_GPUS'])\r\n",
      "    current_host = args.current_host\r\n",
      "    hosts = args.hosts\r\n",
      "    model_dir = args.model_dir\r\n",
      "    CHECKPOINTS_DIR = '/opt/ml/checkpoints'\r\n",
      "    checkpoints_enabled = os.path.exists(CHECKPOINTS_DIR)\r\n",
      "\r\n",
      "    # load training and validation data\r\n",
      "    # we use the gluon.data.vision.MNIST class because of its built in mnist pre-processing logic,\r\n",
      "    # but point it at the location where SageMaker placed the data files, so it doesn't download them again.\r\n",
      "    training_dir = args.train\r\n",
      "    train_data = get_train_data(training_dir + '/train', batch_size)\r\n",
      "    val_data = get_val_data(training_dir + '/test', batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = define_network()\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\r\n",
      "                            {'learning_rate': learning_rate, 'momentum': momentum},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\r\n",
      "\r\n",
      "    # shard the training data in case we are doing distributed training. Alternatively to splitting in memory,\r\n",
      "    # the data could be pre-split in S3 and use ShardedByS3Key to do distributed training.\r\n",
      "    if len(hosts) > 1:\r\n",
      "        train_data = [x for x in train_data]\r\n",
      "        shard_size = len(train_data) // len(hosts)\r\n",
      "        for i, host in enumerate(hosts):\r\n",
      "            if host == current_host:\r\n",
      "                start = shard_size * i\r\n",
      "                end = start + shard_size\r\n",
      "                break\r\n",
      "\r\n",
      "        train_data = train_data[start:end]\r\n",
      "\r\n",
      "    net.hybridize()\r\n",
      "\r\n",
      "    best_val_score = 0.0\r\n",
      "    for epoch in range(epochs):\r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        for i, (data, label) in enumerate(train_data):\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = data.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "                L.backward()\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "            # update metric at last.\r\n",
      "            metric.update([label], [output])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_data)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "        # checkpoint the model, params and optimizer states in the folder /opt/ml/checkpoints\r\n",
      "        if checkpoints_enabled and val_acc > best_val_score:\r\n",
      "            best_val_score = val_acc\r\n",
      "            logging.info('Saving the model, params and optimizer state.')\r\n",
      "            net.export(CHECKPOINTS_DIR + \"/%.4f-gluon_mnist\"%(best_val_score), epoch)\r\n",
      "            trainer.save_states(CHECKPOINTS_DIR + '/%.4f-gluon_mnist-%d.states'%(best_val_score, epoch))\r\n",
      "\r\n",
      "    if current_host == hosts[0]:\r\n",
      "        save(net, model_dir)\r\n",
      "\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    # save the model\r\n",
      "    net.export('%s/model'% model_dir)\r\n",
      "\r\n",
      "\r\n",
      "def define_network():\r\n",
      "    net = nn.HybridSequential()\r\n",
      "    with net.name_scope():\r\n",
      "        net.add(nn.Dense(128, activation='relu'))\r\n",
      "        net.add(nn.Dense(64, activation='relu'))\r\n",
      "        net.add(nn.Dense(10))\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def input_transformer(data, label):\r\n",
      "    data = data.reshape((-1,)).astype(np.float32) / 255.\r\n",
      "    return data, label\r\n",
      "\r\n",
      "\r\n",
      "def get_train_data(data_dir, batch_size):\r\n",
      "    return gluon.data.DataLoader(\r\n",
      "        gluon.data.vision.MNIST(data_dir, train=True, transform=input_transformer),\r\n",
      "        batch_size=batch_size, shuffle=True, last_batch='rollover')\r\n",
      "\r\n",
      "\r\n",
      "def get_val_data(data_dir, batch_size):\r\n",
      "    return gluon.data.DataLoader(\r\n",
      "        gluon.data.vision.MNIST(data_dir, train=False, transform=input_transformer),\r\n",
      "        batch_size=batch_size, shuffle=False)\r\n",
      "\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for data, label in val_data:\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        output = net(data)\r\n",
      "        metric.update([label], [output])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    net = gluon.SymbolBlock.imports(\r\n",
      "        '%s/model-symbol.json' % model_dir,\r\n",
      "        ['data'],\r\n",
      "        '%s/model-0000.params' % model_dir,\r\n",
      "    )\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    # we can use content types to vary input/output handling, but\r\n",
      "    # here we just assume json for both\r\n",
      "    parsed = json.loads(data)\r\n",
      "    nda = mx.nd.array(parsed)\r\n",
      "    output = net(nda)\r\n",
      "    prediction = mx.nd.argmax(output, axis=1)\r\n",
      "    response_body = json.dumps(prediction.asnumpy().tolist()[0])\r\n",
      "    return response_body, output_content_type\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training execution                                           #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def parse_args():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument('--batch-size', type=int, default=100)\r\n",
      "    parser.add_argument('--epochs', type=int, default=10)\r\n",
      "    parser.add_argument('--learning-rate', type=float, default=0.1)\r\n",
      "    parser.add_argument('--momentum', type=float, default=0.9)\r\n",
      "    parser.add_argument('--log-interval', type=float, default=100)\r\n",
      "\r\n",
      "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\r\n",
      "\r\n",
      "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\r\n",
      "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "\r\n",
      "    return parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    args = parse_args()\r\n",
      "\r\n",
      "    train(args)\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training script on SageMaker\n",
    "\n",
    "The ```MXNet``` class allows us to run our training function on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on a single c4.xlarge instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MXNet(\"mnist.py\",\n",
    "          role=role,\n",
    "          train_instance_count=2,\n",
    "          train_instance_type=\"ml.p2.xlarge\",\n",
    "          framework_version=\"1.4.1\",\n",
    "          py_version=\"py3\",\n",
    "          distributions={'parameter_server': {'enabled': True}},\n",
    "          hyperparameters={'batch-size': 100,\n",
    "                           'epochs': 20,\n",
    "                           'learning-rate': 0.1,\n",
    "                           'momentum': 0.9, \n",
    "                           'log-interval': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `MXNet` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-12 04:36:58 Starting - Starting the training job...\n",
      "2019-12-12 04:36:59 Starting - Launching requested ML instances......\n",
      "2019-12-12 04:38:05 Starting - Preparing the instances for training.........\n",
      "2019-12-12 04:39:47 Downloading - Downloading input data...\n",
      "2019-12-12 04:40:19 Training - Downloading the training image..\u001b[32m2019-12-12 04:40:40,809 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[32m2019-12-12 04:40:40,839 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}', 'SM_USER_ENTRY_POINT': 'mnist.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_parameter_server_enabled\":true}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-2', 'SM_MODULE_NAME': 'mnist', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '1', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"mxnet-training-2019-12-12-04-36-58-258\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}', 'SM_USER_ARGS': '[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_BATCH-SIZE': '100', 'SM_HP_LOG-INTERVAL': '100', 'SM_HP_LEARNING-RATE': '0.1', 'SM_HP_EPOCHS': '20', 'SM_HP_MOMENTUM': '0.9'}\u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:40,654 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:40,681 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}', 'SM_USER_ENTRY_POINT': 'mnist.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_parameter_server_enabled\":true}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'mnist', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '1', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2019-12-12-04-36-58-258\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}', 'SM_USER_ARGS': '[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_BATCH-SIZE': '100', 'SM_HP_LOG-INTERVAL': '100', 'SM_HP_LEARNING-RATE': '0.1', 'SM_HP_EPOCHS': '20', 'SM_HP_MOMENTUM': '0.9'}\u001b[0m\n",
      "\u001b[32m2019-12-12 04:40:43,956 sagemaker_mxnet_container.training INFO     Starting distributed training task\u001b[0m\n",
      "\u001b[32m2019-12-12 04:40:44,337 sagemaker-containers INFO     Module mnist does not provide a setup.py. \u001b[0m\n",
      "\u001b[32mGenerating setup.py\u001b[0m\n",
      "\u001b[32m2019-12-12 04:40:44,337 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[32m2019-12-12 04:40:44,338 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[32m2019-12-12 04:40:44,338 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[32m/usr/local/bin/python3.6 -m pip install -U . \u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:43,725 sagemaker_mxnet_container.training INFO     Starting distributed training task\u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:44,069 sagemaker-containers INFO     Module mnist does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:44,069 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:44,069 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:44,070 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/local/bin/python3.6 -m pip install -U . \u001b[0m\n",
      "\u001b[32mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mInstalling collected packages: mnist\n",
      "  Running setup.py install for mnist: started\u001b[0m\n",
      "\u001b[31m    Running setup.py install for mnist: finished with status 'done'\u001b[0m\n",
      "\u001b[31mSuccessfully installed mnist-1.0.0\u001b[0m\n",
      "\u001b[31mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-12-12 04:40:46,253 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 100,\n",
      "        \"log-interval\": 100,\n",
      "        \"learning-rate\": 0.1,\n",
      "        \"epochs\": 20,\n",
      "        \"momentum\": 0.9\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2019-12-12-04-36-58-258\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2019-12-12-04-36-58-258\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_BATCH-SIZE=100\u001b[0m\n",
      "\u001b[31mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[31mSM_HP_LEARNING-RATE=0.1\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[31mSM_HP_MOMENTUM=0.9\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/local/bin/python3.6 -m mnist --batch-size 100 --epochs 20 --learning-rate 0.1 --log-interval 100 --momentum 0.9\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-12-12 04:40:40 Training - Training image download completed. Training in progress.\u001b[32mInstalling collected packages: mnist\n",
      "  Running setup.py install for mnist: started\n",
      "    Running setup.py install for mnist: finished with status 'done'\u001b[0m\n",
      "\u001b[32mSuccessfully installed mnist-1.0.0\u001b[0m\n",
      "\u001b[32mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[32mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[32m2019-12-12 04:40:46,419 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[32mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 100,\n",
      "        \"log-interval\": 100,\n",
      "        \"learning-rate\": 0.1,\n",
      "        \"epochs\": 20,\n",
      "        \"momentum\": 0.9\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"mxnet-training-2019-12-12-04-36-58-258\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[32m}\n",
      "\u001b[0m\n",
      "\u001b[32mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"mxnet-training-2019-12-12-04-36-58-258\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-349934754982/mxnet-training-2019-12-12-04-36-58-258/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[32mSM_HP_BATCH-SIZE=100\u001b[0m\n",
      "\u001b[32mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[32mSM_HP_LEARNING-RATE=0.1\u001b[0m\n",
      "\u001b[32mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[32mSM_HP_MOMENTUM=0.9\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[32m/usr/local/bin/python3.6 -m mnist --batch-size 100 --epochs 20 --learning-rate 0.1 --log-interval 100 --momentum 0.9\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 100] Training: accuracy=0.828020, 12918.668186 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 0 Batch 100] Training: accuracy=0.828317, 13036.720231 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 0 Batch 200] Training: accuracy=0.884030, 17560.410299 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 0 Batch 200] Training: accuracy=0.882488, 17974.304693 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 0] Training: accuracy=0.907000\u001b[0m\n",
      "\u001b[31m[Epoch 0] Training: accuracy=0.904633\u001b[0m\n",
      "\u001b[32m[Epoch 0] Validation: accuracy=0.955500\u001b[0m\n",
      "\u001b[31m[Epoch 0] Validation: accuracy=0.955500\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 100] Training: accuracy=0.957129, 20039.675108 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 1 Batch 100] Training: accuracy=0.957426, 21264.976678 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 1 Batch 200] Training: accuracy=0.963035, 15159.952290 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 1 Batch 200] Training: accuracy=0.962587, 16460.515678 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 1] Training: accuracy=0.966167\u001b[0m\n",
      "\u001b[31m[Epoch 1] Training: accuracy=0.965367\u001b[0m\n",
      "\u001b[32m[Epoch 1] Validation: accuracy=0.961300\u001b[0m\n",
      "\u001b[31m[Epoch 1] Validation: accuracy=0.961300\u001b[0m\n",
      "\u001b[31m[Epoch 2 Batch 100] Training: accuracy=0.972970, 2020.728065 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 2 Batch 100] Training: accuracy=0.974257, 2311.586303 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 2 Batch 200] Training: accuracy=0.975274, 16126.356261 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 2 Batch 200] Training: accuracy=0.974975, 18791.684588 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 2] Training: accuracy=0.976333\u001b[0m\n",
      "\u001b[31m[Epoch 2] Training: accuracy=0.976267\u001b[0m\n",
      "\u001b[31m[Epoch 2] Validation: accuracy=0.966800\u001b[0m\n",
      "\u001b[31m[Epoch 3 Batch 100] Training: accuracy=0.980198, 16978.925637 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 3 Batch 200] Training: accuracy=0.982090, 15173.114351 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 3] Training: accuracy=0.982600\u001b[0m\n",
      "\u001b[32m[Epoch 2] Validation: accuracy=0.966800\u001b[0m\n",
      "\u001b[32m[Epoch 3 Batch 100] Training: accuracy=0.979505, 14255.188118 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 3 Batch 200] Training: accuracy=0.980249, 14692.626195 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 3] Training: accuracy=0.981833\u001b[0m\n",
      "\u001b[32m[Epoch 3] Validation: accuracy=0.969100\u001b[0m\n",
      "\u001b[31m[Epoch 3] Validation: accuracy=0.969100\u001b[0m\n",
      "\u001b[32m[Epoch 4 Batch 100] Training: accuracy=0.980693, 17652.794613 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 4 Batch 200] Training: accuracy=0.981443, 14011.839380 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 4 Batch 100] Training: accuracy=0.981386, 15321.098773 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 4 Batch 200] Training: accuracy=0.982488, 17015.432049 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 4] Training: accuracy=0.982400\u001b[0m\n",
      "\u001b[31m[Epoch 4] Training: accuracy=0.983000\u001b[0m\n",
      "\u001b[32m[Epoch 4] Validation: accuracy=0.968800\u001b[0m\n",
      "\u001b[31m[Epoch 4] Validation: accuracy=0.968800\u001b[0m\n",
      "\u001b[32m[Epoch 5 Batch 100] Training: accuracy=0.985941, 12113.513357 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 5 Batch 200] Training: accuracy=0.985672, 13379.386902 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5 Batch 100] Training: accuracy=0.984455, 12231.500977 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5 Batch 200] Training: accuracy=0.985572, 12454.506043 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 5] Training: accuracy=0.985433\u001b[0m\n",
      "\u001b[31m[Epoch 5] Validation: accuracy=0.970400\u001b[0m\n",
      "\u001b[31m[Epoch 6 Batch 100] Training: accuracy=0.988317, 20112.707394 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 6 Batch 200] Training: accuracy=0.988259, 12138.052380 samples/s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[Epoch 5] Training: accuracy=0.986100\u001b[0m\n",
      "\u001b[32m[Epoch 5] Validation: accuracy=0.970400\u001b[0m\n",
      "\u001b[32m[Epoch 6 Batch 100] Training: accuracy=0.989307, 16137.524528 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 6 Batch 200] Training: accuracy=0.989154, 11684.925477 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 6] Training: accuracy=0.989067\u001b[0m\n",
      "\u001b[31m[Epoch 6] Training: accuracy=0.988300\u001b[0m\n",
      "\u001b[32m[Epoch 6] Validation: accuracy=0.968600\u001b[0m\n",
      "\u001b[31m[Epoch 6] Validation: accuracy=0.968600\u001b[0m\n",
      "\u001b[32m[Epoch 7 Batch 100] Training: accuracy=0.988515, 9113.099402 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 7 Batch 200] Training: accuracy=0.989900, 15359.250037 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 7 Batch 100] Training: accuracy=0.989010, 12792.192265 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 7 Batch 200] Training: accuracy=0.989154, 14271.194284 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 7] Training: accuracy=0.989367\u001b[0m\n",
      "\u001b[31m[Epoch 7] Training: accuracy=0.988633\u001b[0m\n",
      "\u001b[32m[Epoch 7] Validation: accuracy=0.970700\u001b[0m\n",
      "\u001b[31m[Epoch 7] Validation: accuracy=0.970700\u001b[0m\n",
      "\u001b[32m[Epoch 8 Batch 100] Training: accuracy=0.990297, 20188.217174 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 8 Batch 100] Training: accuracy=0.990099, 17906.775392 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 8 Batch 200] Training: accuracy=0.989851, 11624.044564 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 8 Batch 200] Training: accuracy=0.989701, 11298.397220 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 8] Training: accuracy=0.989767\u001b[0m\n",
      "\u001b[31m[Epoch 8] Training: accuracy=0.989900\u001b[0m\n",
      "\u001b[32m[Epoch 8] Validation: accuracy=0.970700\u001b[0m\n",
      "\u001b[31m[Epoch 8] Validation: accuracy=0.970700\u001b[0m\n",
      "\u001b[32m[Epoch 9 Batch 100] Training: accuracy=0.990099, 15287.035755 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 9 Batch 100] Training: accuracy=0.992079, 16997.503647 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 9 Batch 200] Training: accuracy=0.991741, 15462.301851 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 9 Batch 200] Training: accuracy=0.990398, 18517.898455 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 9] Training: accuracy=0.990067\u001b[0m\n",
      "\u001b[31m[Epoch 9] Training: accuracy=0.990900\u001b[0m\n",
      "\u001b[32m[Epoch 9] Validation: accuracy=0.968300\u001b[0m\n",
      "\u001b[31m[Epoch 9] Validation: accuracy=0.968300\u001b[0m\n",
      "\u001b[32m[Epoch 10 Batch 100] Training: accuracy=0.991485, 15949.136816 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 10 Batch 100] Training: accuracy=0.992376, 15935.200030 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 10 Batch 200] Training: accuracy=0.991791, 17192.588949 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 10 Batch 200] Training: accuracy=0.991045, 15478.850057 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 10] Training: accuracy=0.991400\u001b[0m\n",
      "\u001b[31m[Epoch 10] Training: accuracy=0.991467\u001b[0m\n",
      "\u001b[32m[Epoch 10] Validation: accuracy=0.971300\u001b[0m\n",
      "\u001b[31m[Epoch 10] Validation: accuracy=0.971300\u001b[0m\n",
      "\u001b[32m[Epoch 11 Batch 100] Training: accuracy=0.992673, 17470.443186 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 11 Batch 100] Training: accuracy=0.991584, 17093.793047 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 11 Batch 200] Training: accuracy=0.992637, 15430.446619 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 11 Batch 200] Training: accuracy=0.993483, 14492.101444 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 11] Training: accuracy=0.993400\u001b[0m\n",
      "\u001b[31m[Epoch 11] Training: accuracy=0.992700\u001b[0m\n",
      "\u001b[32m[Epoch 11] Validation: accuracy=0.970200\u001b[0m\n",
      "\u001b[31m[Epoch 11] Validation: accuracy=0.970200\u001b[0m\n",
      "\u001b[32m[Epoch 12 Batch 100] Training: accuracy=0.993960, 15773.397014 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 12 Batch 100] Training: accuracy=0.994158, 16828.374258 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 12 Batch 200] Training: accuracy=0.994080, 17225.067762 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 12 Batch 200] Training: accuracy=0.993532, 17097.277026 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 12] Training: accuracy=0.992233\u001b[0m\n",
      "\u001b[31m[Epoch 12] Training: accuracy=0.993400\u001b[0m\n",
      "\u001b[32m[Epoch 12] Validation: accuracy=0.972400\u001b[0m\n",
      "\u001b[31m[Epoch 12] Validation: accuracy=0.972400\u001b[0m\n",
      "\u001b[32m[Epoch 13 Batch 100] Training: accuracy=0.992673, 18301.352649 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 13 Batch 100] Training: accuracy=0.993861, 19324.137296 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 13 Batch 200] Training: accuracy=0.993333, 18768.980176 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 13 Batch 200] Training: accuracy=0.992736, 18213.930867 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 13] Training: accuracy=0.992967\u001b[0m\n",
      "\u001b[31m[Epoch 13] Training: accuracy=0.993567\u001b[0m\n",
      "\u001b[32m[Epoch 13] Validation: accuracy=0.975200\u001b[0m\n",
      "\u001b[31m[Epoch 13] Validation: accuracy=0.975200\u001b[0m\n",
      "\u001b[32m[Epoch 14 Batch 100] Training: accuracy=0.996040, 16486.395975 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 14 Batch 100] Training: accuracy=0.994653, 17129.396390 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 14 Batch 200] Training: accuracy=0.995522, 17566.293923 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 14] Training: accuracy=0.995533\u001b[0m\n",
      "\u001b[31m[Epoch 14 Batch 200] Training: accuracy=0.994826, 17963.527346 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 14] Training: accuracy=0.994733\u001b[0m\n",
      "\u001b[32m[Epoch 14] Validation: accuracy=0.967900\u001b[0m\n",
      "\u001b[31m[Epoch 14] Validation: accuracy=0.967900\u001b[0m\n",
      "\u001b[32m[Epoch 15 Batch 100] Training: accuracy=0.993960, 20921.308859 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 15 Batch 100] Training: accuracy=0.992178, 20630.091978 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 15 Batch 200] Training: accuracy=0.993881, 20195.021426 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 15 Batch 200] Training: accuracy=0.993980, 20023.411467 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 15] Training: accuracy=0.994400\u001b[0m\n",
      "\u001b[31m[Epoch 15] Training: accuracy=0.994533\u001b[0m\n",
      "\u001b[32m[Epoch 15] Validation: accuracy=0.973500\u001b[0m\n",
      "\u001b[31m[Epoch 15] Validation: accuracy=0.973500\u001b[0m\n",
      "\u001b[32m[Epoch 16 Batch 100] Training: accuracy=0.995446, 16718.367347 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 16 Batch 200] Training: accuracy=0.995572, 17517.139993 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 16 Batch 100] Training: accuracy=0.994257, 15432.717639 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 16 Batch 200] Training: accuracy=0.994975, 16441.803214 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 16] Training: accuracy=0.996067\u001b[0m\n",
      "\u001b[31m[Epoch 16] Training: accuracy=0.995600\u001b[0m\n",
      "\u001b[32m[Epoch 16] Validation: accuracy=0.973400\u001b[0m\n",
      "\u001b[31m[Epoch 16] Validation: accuracy=0.973400\u001b[0m\n",
      "\u001b[32m[Epoch 17 Batch 100] Training: accuracy=0.995842, 17013.361457 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 17 Batch 200] Training: accuracy=0.995821, 16961.074043 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 17 Batch 100] Training: accuracy=0.995644, 17326.107072 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 17 Batch 200] Training: accuracy=0.996119, 17307.518363 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 17] Training: accuracy=0.996033\u001b[0m\n",
      "\u001b[31m[Epoch 17] Training: accuracy=0.996567\u001b[0m\n",
      "\u001b[32m[Epoch 17] Validation: accuracy=0.976800\u001b[0m\n",
      "\u001b[31m[Epoch 17] Validation: accuracy=0.976800\u001b[0m\n",
      "\u001b[32m[Epoch 18 Batch 100] Training: accuracy=0.997525, 15822.785574 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 18 Batch 200] Training: accuracy=0.997861, 12163.749202 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 18 Batch 100] Training: accuracy=0.997921, 15541.366533 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 18 Batch 200] Training: accuracy=0.998010, 12789.851802 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 18] Training: accuracy=0.998033\u001b[0m\n",
      "\n",
      "2019-12-12 04:42:28 Uploading - Uploading generated training model\u001b[31m[Epoch 18] Training: accuracy=0.998267\u001b[0m\n",
      "\u001b[32m[Epoch 18] Validation: accuracy=0.976100\u001b[0m\n",
      "\u001b[31m[Epoch 18] Validation: accuracy=0.976100\u001b[0m\n",
      "\u001b[32m[Epoch 19 Batch 100] Training: accuracy=0.998317, 11595.443990 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 19 Batch 200] Training: accuracy=0.998060, 11728.710047 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 19 Batch 100] Training: accuracy=0.997822, 11307.230280 samples/s\u001b[0m\n",
      "\u001b[31m[Epoch 19 Batch 200] Training: accuracy=0.998010, 13934.564784 samples/s\u001b[0m\n",
      "\u001b[32m[Epoch 19] Training: accuracy=0.997500\u001b[0m\n",
      "\u001b[31m[Epoch 19] Training: accuracy=0.997400\u001b[0m\n",
      "\u001b[32m[Epoch 19] Validation: accuracy=0.974400\u001b[0m\n",
      "\u001b[31m[Epoch 19] Validation: accuracy=0.974400\u001b[0m\n",
      "\u001b[31m2019-12-12 04:42:23,531 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32m2019-12-12 04:42:23,601 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we use the MXNet object to build and deploy an MXNetPredictor object. This creates a SageMaker endpoint that we can use to perform inference. \n",
    "\n",
    "This allows us to perform inference on json encoded multi-dimensional arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a 'data' variable in this notebook, which we can then pass to the mxnet predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor runs inference on our input data and returns the predicted digit (as a float value, so we convert to int for display)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = predictor.predict(data)\n",
    "print(int(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
